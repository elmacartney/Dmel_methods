---
title: "Zantiks manipulation scripts"
author: "Szymek Drobniak, Erin Macartney & Shinichi Nakagawa"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
      
      code_folding: hide
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F}
library(tidyverse)
library(lme4)
library(lmerTest)
library(MCMCglmm)
library(here) # write out the path 
```

# Locomotion

## Data parsing

First let's test-load one data file to see how to trim it into relevant bits. Definition in this files is so that first 4 rows and firts 6 columns are redundant.

The first portion loads and parses the raw data file lines.

```{r}
data_path <- here("Data/locomotion/")
data_files <- list.files(here("Data/locomotion/"), recursive = T)
head(data_files)
```

Currently each file represents a set of uniquely analyzed individuals and hence no additional complications arise (like, e.g., having two files with the same individual assayed on two occasions).

In the next step we test the approach on several steps. First - we process a sample CSV file, which is unstructured and contains a lot of spurious lines and data (e.g., control variables and comments generated by the unit).

```{r}
# read in one specific file in mac
dat_temp <- readLines(paste(data_path, data_files[1], sep = ''))
glimpse(dat_temp)
```

Loaded data is just a vector of strings, each being a line from the original CSV file. In the next chunk we skip the first 4 lines (fixed number, lines containing technical parameters of the units), and load 5 lines (which excludes the header line - so in fact 6 lines), skipping the last technical line. At the end we clean the file (removing empty columns `2:6` and wells `c(F6, F7, F8)` - they are always empty in our system). Adjust as needed. The resulting file contains each assayed well as separate column, and for each there are 5 times intervals of locomotion monitoring.

```{r}
dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                        quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                        stringsAsFactors = F)

# altearntive should work too
#dat_temp_df2 <- read.csv(file = here("Data", "locomotion", data_files[2]), header = T,  skip = 4)

dat_temp_df <- dat_temp_df[, -(2:6)]
dat_temp_df <- dat_temp_df %>% select(!(F6:F8))
dat_temp_df

glimpse(dat_temp_df)
```

Here we extract and append the run (subject) ID.

```{r}
# // the below code was seriously messed up by sb - fixing it
# // plz do not use the tidyverse read commands unless it's absolutely necessary
# // they introduce bugs more often than help

head_temp <- readLines(paste(data_path, data_files[1], sep = ''), n = 4)

# using RE - this method is more flexible as the structure and location of ID can change
id_index <- grep('Subject Identification', head_temp)
id_index <- grep('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', head_temp)
run_id <- gsub('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', '\\1', head_temp[id_index])
dat_temp_df$Datafile_ID <- run_id

# // the below code did not work - not sure what was the idea?
# dat_temp_df$run_id  <- head_temp[[4]][1]
# run_id <- head_temp[[4]][1]
# assay_date <- str_sub(run_id, 11, 16)

assay_date <- str_sub(run_id, 11, 16)
dat_temp_df$date <- assay_date
glimpse(dat_temp_df)
```

Change to long format. Now each well x time combination has it's separate row, and since we have datafile ID we can link wells to specific individuals - e.g., to link-in sex information.

```{r}
dat_temp_df <-
  dat_temp_df %>%
  pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]'))
#  unite(col = 'indiv_id', c(run_id, well_id), sep = '_', remove = F) 

head(dat_temp_df)
```

We have to add the `Individuals_ID` variable to be able to link locomotion data to sex data.

```{r}
run_register <- read_delim(here('Data', 'sex', 'run_register.csv'), delim = ';')
glimpse(run_register)

dat_temp_df <- dat_temp_df %>%
  left_join(select(run_register, Datafile_ID, Individuals_ID))

dat_temp_df <- dat_temp_df %>%
    mutate(Individuals_ID_well = paste0(Individuals_ID, '_', well_id))
glimpse(dat_temp_df)
```

Writing a function and using map to apply data loading and wrangling to all files. Note: the function uses the pattern found in text files so an important point - do not resave Zantiks CSV files with any external software (be it MS Excel, Numbers, Google Sheets, etc.) as it will mess up with the formatting and the way text strings are coded. Adjust internal REs whenever needed.

```{r}

# // plz, PLZ do not write functions before the algorithm is tested - debugging is much more difficult!
# // I'm rewriting the below using my mods in previous section
# // many lines generated errors - no time to follow them, my guess they arise becasue formatting of data
# // changes - RE are much more flexible and do not require using high-level substring functions

data_compiler <- function(filename, data_path, run_register) {
  
  # // plz document function arguments
  ## ARG filename     vector or list of file names to process (only files, no full paths)
  ## ARG data_path    text string with the path to access all files
  ## ARG run_register database of all runs with datafile IDs and sexing IDs
  
  dat_temp <- readLines(paste(data_path, filename, sep = ''))
  
  # file parsing
  dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                          quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                          stringsAsFactors = F)
  
  dat_temp_df <- dat_temp_df[, -(2:6)]
  dat_temp_df <- dat_temp_df %>% select(!(F6:F8))
  
  # extract headers
  head_temp <- readLines(paste(data_path, filename, sep = ''), n = 4)
  
  # using RE - this method is more flexible as the structure and location of ID can change
  id_index <- grep('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', head_temp)
  run_id <- gsub('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', '\\1', head_temp[id_index])
  dat_temp_df$Datafile_ID <- run_id
  
  assay_date <- str_sub(run_id, 11, 16)
  dat_temp_df$date <- assay_date
  
  dat_temp_df <- dat_temp_df %>%
    pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]'))
  
  dat_temp_df <- dat_temp_df %>%
    left_join(select(run_register, Datafile_ID, Individuals_ID))
  
  dat_temp_df <- dat_temp_df %>%
    mutate(Individuals_ID_well = paste0(Individuals_ID, '_', well_id))
  
  dat_temp_df$filename <- filename
  
  return(dat_temp_df)
  
}
```

Process the files using the function, mapping it row-wise into a now data-frame.

```{r message = F}
# Not run: test
# data_compiler(data_files[1], data_path, run_register)
# map! // map creates unclosed terminals to files handled by file read functions!! for loop is much safer

data_locomotion <- map_dfr(data_files, ~ data_compiler(.x, data_path = data_path, run_register = run_register))
# current test excludes some files as they were misformatted - to FIX

glimpse(data_locomotion)
```

## Adding sex information

Below we load the sex information (referenced to our data via the `Individuals_ID` data from the run registry) and left-join it with the locomotion data.

```{r}

sex <- read_csv(here("Data", "sex", "Run register_Pilot 2 - Sex.csv"))


sex <- sex %>%
  mutate(Individuals_ID_well = paste0(Indviduals_ID, "_", Wellplate_location))

# join together

data_locomotion <- data_locomotion %>%
  left_join(sex, by = "Individuals_ID_well")

```



## Example visualisations nad basic summaries

```{r}
data_locomotion_avg <- data_locomotion %>%
  group_by(Individuals_ID_well) %>%
  summarise(avg = mean(arena_distance))

# raw data variation
data_locomotion_avg %>%
  ggplot(aes(x = avg)) +
  geom_histogram() +
  theme_classic() + theme(text = element_text(size = 15)) +
  xlab("Mean arena distance")

# log scale
data_locomotion_avg %>%
  ggplot(aes(x = log(avg+1))) +
  geom_histogram() +
  theme_classic() + theme(text = element_text(size = 15)) +
  xlab("Mean log(arena distance)")

data_locomotion %>%
  ggplot(aes(x = Individuals_ID_well, y = log(arena_distance+1))) +
  geom_boxplot(width = 0, outlier.size = 0.7) + theme_classic() + theme(axis.text.x = element_blank()) +
  ylab("Arena distance") + xlab("Individuals")
```

We can also visualise sexual differences.

```{r}

#violin plot of distance grouped by sex
library(ggpubr)

# dat_df3<- na.omit(dat_df2)

violin <- data_locomotion %>% 
  group_by(Individuals_ID_well) %>% 
  summarise(arena_distance = log(mean(arena_distance) +1), Sex = unique(Sex)) %>% 
  ggviolin(., x = "Sex", y = "arena_distance", add = c("jitter", "mean_se"), error.plot = "crossbar", palette= NULL) +
  labs(y = "log(Mean arena distance)")
violin
#some huge outliers in females
pos_female <- which.max(data_locomotion$arena_distance)
data_locomotion[pos_female, ]

# //not sure what this was for?
# dat_df3[dat_df3$indiv_id == "ID_210421_3_B5", ]

#scatter plot of distance and temp // invalid, we have not varied temperature
# plot <- ggplot(data_locomotion, aes(x = TEMPERATURE, y = log(arena_distance+1))) + geom_point() + geom_smooth(method = lm)


#simple lmer
mod1 <- lmer(log(arena_distance+1) ~ Sex + as.factor(date) + (1|Individuals_ID_well) + (1|Datafile_ID),
             data = data_locomotion)
summary(mod1) # no difference between sex or temperature
hist(residuals(mod1), breaks = 20)

# random intercept model
mod2 <- lmer(log(arena_distance+1) ~ Sex + as.factor(date) + ROUND + (1|Individuals_ID_well) + (1|Datafile_ID),
             data = data_locomotion)
summary(mod2)

# random slope model
mod3 <- lmer(log(arena_distance+1) ~ Sex + as.factor(date) + ROUND + (1 + ROUND|Individuals_ID_well) + (1|Datafile_ID),
             data = data_locomotion)
summary(mod3)
#round is important as they move more during the assay. This increase differs between individuals


```

## Mixed-model with between-individual variation

```{r}
model1mc <- MCMCglmm(log(arena_distance+1) ~ Sex + ROUND + as.factor(date),
                     random = ~ Individuals_ID_well + Datafile_ID,
                     data = na.omit(as.data.frame(data_locomotion)), verbose = F,
                     prior = list(R = list(V = 1, nu = 0.002),
                                  G = list(G1 = list(V = 1, nu = 0.002, alpha.V = 0, alpha.mu = 1e4),
                                           G2 = list(V = 1, nu = 0.002, alpha.V = 0, alpha.mu = 1e4))),
                     nitt = 5e4, burnin = 1e4, thin = 40)
# plot(model1mc)
mean(model1mc$VCV[,"Individuals_ID_well"]/rowSums(model1mc$VCV))
HPDinterval(model1mc$VCV[,"Individuals_ID_well"]/rowSums(model1mc$VCV))
```


# Y-maze

## Data loading

First we load the y-maze data and tidy it up to separate summary data from zone changes data.

```{r}
# parse datafiles into a nested tibble
data_path <- here('Data/ymaze/')

#data_path <- paste0(data_path, "/") #need this when using PC. Don't use on Mac
data_files <- list.files(here('Data/ymaze/'), recursive = T)
head(data_files)

# helper functions extracting the data rows and the header rows
data_parser <- function(pattern, path, filename, summary = FALSE) {
  dat_temp <- readLines(paste(path, filename, sep = ''))
  zone_change_index <- grep(pattern, dat_temp)
  
  if(summary == FALSE) {
    return(read_delim(file = dat_temp[zone_change_index], col_names = F, delim = ',',
                      quote = '\"' # skip = 4, nrows = length(dat_temp) - 4 - 1)
    ))
  } else {
    return(read_delim(file = dat_temp[-zone_change_index], col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3))
  }
}

# test
pattern1 <- "Arena"
test1 <- readLines(paste(data_path, data_files[1], sep = ''))
id_test <- grep(pattern1, test1)
test_dat <- read_delim(file = test1[-id_test], col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3)


head_parser <- function(path, filename) {
  dat_temp <- readLines(paste(path, filename, sep = ''), n = 4)
  return(read_delim(file = dat_temp,
                    delim = ',',
                    col_names = F))
}
```

The below code generates a nested tibble with respective filenames grouping respective arena and header data.
```{r}
# create tibble with raw data (nested: data and header nested under file names)
dat_raw <- tibble(data_files = data_files) %>%
  mutate(data = map(data_files, 
                    ~ data_parser(pattern1,
                                  data_path,
                                  .x, summary = FALSE))) %>%
  mutate(head = map(data_files, ~ head_parser(data_path, .x)))

dat_raw[[2]] = dat_raw[[2]] %>%
  map(~ select(.x, !c(X3)))

# rename variables in sub-tibble
dat_raw[[2]] = dat_raw[[2]] %>%
  map(~ rename(.x, time = X1, info = X2, arena = X4, action = X5, zone_no = X6))

# reformat the head sub-tibble
dat_raw[[3]] = dat_raw[[3]] %>%
  map(~ pivot_wider(.x, names_from = X3, values_from = X4)) %>%
  map(~ select(.x, 'X1', 'X2', 'Apparatus', 'Subject Identification'))

# check the modifications
dat_raw[[2]][[1]]
dat_raw[[3]][[1]]


# parse summary data from a file into a separate tibble
dat_raw_summ = tibble(data_files = data_files) %>%
  mutate(data = map(data_files, 
                    ~ #data_parser('[0-9]{3,4}.[0-9]{3},\\"Info\\",\\"Arena\\",[0-9]{1,2},\\"(?:Enter|Exit)_Zone\\",[0-9]',
                      data_parser(pattern1,
                                data_path,
                                .x,
                                summary = T)))
dat_raw_summ[[2]] = dat_raw_summ[[2]] %>%
  map(~ select(.x, !c(X2, X3)))
dat_raw_summ[[2]][[1]]
```

Unnest the tibble and select relevant variables.

```{r}
dat_un <- dat_raw
dat_un = dat_raw %>%
  unnest(head) %>%
  unnest(data)

dat_un <- dat_un %>%
  select(data_files, `Subject Identification`, time, arena, action, zone_no) %>%
  rename(run_id = `Subject Identification`)

```

Turn into wider format with enter/exit columns, add row ID to maintain order if needed.

```{r}
dat_un <- dat_un %>%
  mutate(row_id = row_number()) %>%
  pivot_wider(names_from = action, values_from = zone_no) %>%
  rename(enter_zone = Enter_Zone, exit_zone = Exit_Zone)
dat_un
```

Bin into 10-min intervals (if binning would be needed or only first 10 mins of exploration used). Then sort the dataset into individual arenas and chronologically within arenas, and then in enter-exit order at the lowest level.

```{r}
dat_an <- dat_un

dat_an <- dat_an %>%
  mutate(bin = ifelse(time > 600 & time < 1200, 1,
                      ifelse(time > 1200 & time < 1800, 2,
                             ifelse(time > 1800 & time < 2500, 3, NA))))

dat_an <- dat_an %>%
  arrange(data_files, run_id, arena, time, exit_zone)
dat_an
```

Error checking - do all enter zones have a matching exit zone (if enter and exit exist)?

```{r}
dat_an <- dat_an %>%
  mutate(zone = ifelse(enter_zone == lead(exit_zone), enter_zone, 666)) %>%
  mutate(t_enter = ifelse(enter_zone >= 1, time, 666)) %>%
  mutate(t_exit = ifelse(exit_zone >= 1, time, 666))

dat_an %>% filter(zone == 666)
dat_an %>% filter(t_enter == 666)
dat_an %>% filter(t_exit == 666)
# ALL GOOD! Note - this step is v. important and serves to test
# if datapoints were sorted correctly
```

Simplify data by putting enter and exit data in one row:

```{r}
dat_an <- dat_an %>%
  filter(is.na(zone) | zone != 666) %>%
  select(data_files, run_id, time, arena, row_id, bin, zone, t_enter, t_exit)
dat_an <- dat_an %>%
  mutate(t_exit = lead(t_exit))
dat_an <- na.omit(dat_an)
dat_an <- dat_an %>%
  mutate(t_zone = t_exit - t_enter)

dat_an
```

Remove center zone (middle of the Y-maze).

```{r}
dat_an2 <- dat_an %>% filter(zone != 4)
dat_an2
```

We need to work separately for each individual - the easiest is to separate individuals into their own subtibbles and work on them by vectorizing operations - to achieve this we have to identify each individual (by pasting together file name and arena).

```{r}
well_arena <- read_delim(here('Data', 'sex', 'well_arena_corresp.csv'), delim = ';')

names(dat_an2)[2] <- "Datafile_ID"
dat_an2 <- dat_an2 %>%
  left_join(select(run_register, Datafile_ID, Individuals_ID, Plate, Unit_ID), by = "Datafile_ID") %>%
  mutate(ymaze_arena = paste0(Plate, "_", arena)) %>%
  # mutate(fly_id = paste(gsub('[A-Za-z0-9]+\\/([A-Za-z0-9_-]+)\\.csv', '\\1', data_files), arena, sep = "_"))
  mutate(fly_id = paste0(Datafile_ID, "_", ymaze_arena)) %>%
  left_join(well_arena, by = "ymaze_arena")

# split datafile into individual flies
dat_an2 <- dat_an2 %>% split(., .[, "fly_id"])

dat_an2 <- dat_an2 %>%
  map(~ mutate(.x, lag_zone = lag(zone))) %>%
  map(~ mutate(.x, turn = case_when(lag_zone==1 & zone==2 ~ 'L',
                                  lag_zone==1 & zone==3 ~ 'R',
                                  lag_zone==2 & zone==1 ~ 'R',
                                  lag_zone==2 & zone==3 ~ 'L',
                                  lag_zone==3 & zone==1 ~ 'L',
                                  lag_zone==3 & zone==2 ~ 'R',
                                  # lag_zone==zone ~ 'X',
                                  TRUE~ NA_character_ ))) %>%
  map(~ select(.x, data_files, Datafile_ID, fly_id, time, ymaze_arena, Individuals_ID, plate48well, bin, zone, t_zone, turn))

dat_an3 <- bind_rows(dat_an2)
dat_an3 <- dat_an3 %>%
  arrange(data_files, Datafile_ID, fly_id, bin)

dat_an4 <- na.omit(dat_an3)
dat_an4
```

Analysis of tetragrams.

```{r}
dat_tet <- dat_an4 %>%
  group_by(fly_id)

dat_tet <- dat_tet %>%
  mutate(tetragram = str_c(turn, lead(turn), lead(turn,2), lead(turn,3))) %>%
  ungroup() %>%
  select(fly_id, turn, tetragram)
dat_tet
```

We can now summarise tetragrams and turns:

```{r}
all_tetragrams <- unique(dat_tet$tetragram)

tetra_long <- dat_tet %>%
  select(-turn) %>%
  na.omit() %>%
  group_by(fly_id) %>%
  table() %>%
  as_tibble() %>%
  arrange(fly_id)

tetra_wide = tetra_long %>% pivot_wider(names_from = tetragram, values_from = n)

turn_long = dat_tet %>%
  select(-tetragram) %>%
  na.omit() %>%
  group_by(fly_id) %>%
  table() %>%
  as_tibble() %>%
  arrange(fly_id)

turn_wide = turn_long %>% pivot_wider(names_from = turn, values_from = n)

data = tetra_wide %>%
  left_join(turn_wide, by = c("fly_id")) %>%
  mutate(total_turns = L+R,
         reps = LLLL + RRRR,
         alter = RLRL + LRLR,
         rel_reps = (reps*100)/total_turns,
         rel_alter = (alter*100)/total_turns,
         rel_R = (R*100)/total_turns,
         rel_L = (L*100)/total_turns)

data
```

## Linking to sex data - NOT FINISHED

```{r}


data <- data %>%
  left_join(run_register)

```

## Example visualisations of y-maze data

```{r}

tetra_long %>%
  filter(n > 0) %>%
  ggplot(aes(x = tetragram, y = n)) +
  geom_bar(stat = "sum") +
  theme_classic() + labs(x = 'Tetragram', y = 'Total number') +
  theme(legend.position = "none", text = element_text(size = 15))



```