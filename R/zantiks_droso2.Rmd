---
title: "Zantiks manipulation scripts"
author: "Szymek Drobniak, Erin Macartney & Shinichi Nakagawa"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
      
      code_folding: hide
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F}
library(tidyverse)
library(lme4)
library(lmerTest)
library(here) # write out the path
library(kableExtra)
```

# Locomotion

## Data parsing

First let's test-load one data file to see how to trim it into relevant bits. Definition in this files is so that first 4 rows and firts 6 columns are redundant.

The first portion loads and parses the raw data file lines.

```{r}
data_path <- here("Data/locomotion/")
data_files <- list.files(here("Data/locomotion/"), recursive = T)
data_files[1:8] # the first 8 files
```

Currently each file represents a set of uniquely analyzed individuals and hence no additional complications arise (like, e.g., having two files with the same individual assayed on two occasions).

In the next step we test the approach on several steps. First - we process a sample CSV file, which is unstructured and contains a lot of spurious lines and data (e.g., control variables and comments generated by the unit).

```{r}
# read in one specific file
dat_temp <- readLines(paste(data_path, data_files[1], sep = ''))
glimpse(dat_temp)
```

The loaded data is just a vector of strings, each being a line from the original CSV file. In the next chunk we skip the first 4 lines (fixed number, lines containing technical parameters of the units), and load 5 lines (which excludes the header line - so in fact 6 lines), skipping the last technical line. At the end we clean the file (removing empty columns `2:6` and wells `c(F6, F7, F8)` - they are always empty in our system). Adjust as needed. The resulting file contains each assayed well as separate column, and for each there are 5 times intervals of locomotion monitoring.

```{r}
dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                        quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                        stringsAsFactors = F)

dat_temp_df <- dat_temp_df[, -(2:6)] # remove spurious columns
dat_temp_df <- dat_temp_df %>% select(!(F6:F8)) #remove wells that do not contain data
dat_temp_df

glimpse(dat_temp_df)
```

Here we extract and append the run (subject) ID.

```{r}
head_temp <- readLines(paste(data_path, data_files[1], sep = ''), n = 4)

# using RE - this method is more flexible as the structure and location of ID can change

# both of the below definitions will work, but second is more precise
# it uses the exact format of the run ID

id_index <- grep('Subject Identification', head_temp)
id_index <- grep('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', head_temp)
run_id <- gsub('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', '\\1', head_temp[id_index])
dat_temp_df$Datafile_ID <- run_id

assay_date <- str_sub(run_id, 11, 16)
dat_temp_df$date <- as.Date(assay_date, format = "%y%m%d")
glimpse(dat_temp_df)
```

Change to long format. Now each well x time combination has it's separate row, and since we have datafile ID we can link wells to specific individuals - e.g., to link-in sex information. We also rename variable according to convention: categorical variables - `Names_start_with_upper_case`; continuous variables - `all_lower_case`.

```{r}
dat_temp_df <-
  dat_temp_df %>%
  pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]')) %>%
  rename(time = TIME, temperature = TEMPERATURE, round = ROUND,
         Variable = VARIABLE, Date = date, Well_ID = well_id)

head(dat_temp_df)
```

We have to add the `Individuals_ID` variable to be able to link locomotion data to sex data. It will serve to link Datafile_ID with respective Individuals_ID - and through it with appropriate well number. The run register contains several variables used to group individuals into several blocks that may share some of the systematic variation.

```{r}
run_meta <- read_delim(here('Data', 'run_data', 'ID_metadata.csv'), delim = ';')
run_meta %>%
  kbl() %>% kable_material(c('striped'))
```


```{r}
run_register <- read_delim(here('Data', 'run_data', 'run_register_pilot2.csv'), delim = ';')
glimpse(run_register)

dat_temp_df <- dat_temp_df %>%
  left_join(select(run_register, Datafile_ID, Individuals_ID))

dat_temp_df <- dat_temp_df %>%
    mutate(Individuals_ID_well = paste0(Individuals_ID, '_', Well_ID))

dat_temp_df <- dat_temp_df %>% select(Individuals_ID_well, Individuals_ID, Datafile_ID, Date,
                                      temperature, round, arena_distance)
glimpse(dat_temp_df)
```

Writing a function and using `map` to apply data loading and wrangling to all files. Note: the function uses the pattern found in text files so an important point - do not re-save Zantiks CSV files with any external software (be it MS Excel, Numbers, Google Sheets, etc.) as it will mess up with the formatting and the way text strings are coded. Adjust REs accordingly to adapt it to your file structure whenever needed.

```{r}
data_compiler <- function(filename, data_path, run_register) {
  
  ## ARG filename     vector or list of file names to process (only files, no full paths)
  ## ARG data_path    text string with the path to access all files
  ## ARG run_register database of all runs with datafile IDs and sexing IDs
  
  ## ARGS to develop: passing custom RE, custom wells to skip, custom columns to skip
  
  dat_temp <- readLines(paste(data_path, filename, sep = ''))
  
  # file parsing
  dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                          quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                          stringsAsFactors = F)
  
  ## these line are design specific - for now the function has close form on those
  ## possible to implement as additional argument
  
  dat_temp_df <- dat_temp_df[, -(2:6)]
  dat_temp_df <- dat_temp_df %>% select(!(F6:F8))
  
  # extract headers
  head_temp <- readLines(paste(data_path, filename, sep = ''), n = 4)
  
  # using RE - this method is more flexible as the structure and location of ID can change
  id_index <- grep('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', head_temp)
  run_id <- gsub('.*Subject Identification\\\",\"([A-Z0-9]{7}_B_[0-9]{6}_[0-9]{1,2})\\\"', '\\1', head_temp[id_index])
  dat_temp_df$Datafile_ID <- run_id
  
  assay_date <- str_sub(run_id, 11, 16)
  dat_temp_df$date <- assay_date
  
  dat_temp_df <-
  dat_temp_df %>%
  pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]')) %>%
  rename(time = TIME, temperature = TEMPERATURE, round = ROUND,
         Variable = VARIABLE, Date = date, Well_ID = well_id)
  
  dat_temp_df <- dat_temp_df %>%
    left_join(select(run_register, Datafile_ID, Individuals_ID))
  
  dat_temp_df <- dat_temp_df %>%
    mutate(Individuals_ID_well = paste0(Individuals_ID, '_', Well_ID))
  
  dat_temp_df$Filename <- filename
  
  dat_temp_df <- dat_temp_df %>% select(Individuals_ID_well, Individuals_ID, Datafile_ID, Date, Filename,
                                      temperature, round, arena_distance)
  
  return(dat_temp_df)
}
```

Process the files using the function, mapping it row-wise into a new data-frame.

```{r message = F}
# Not run: test
# data_compiler(data_files[1], data_path, run_register)

data_locomotion <- map_dfr(data_files, ~ data_compiler(.x, data_path = data_path, run_register = run_register))

length(unique(data_locomotion$Datafile_ID)) # should be 8 distinct files

glimpse(data_locomotion)
```

## Adding sex information

Below we load the sex information (referenced to our data via the `Individuals_ID` data from the sex registry file) and left-join it with the locomotion data.

```{r}

sex <- read_csv(here("Data", "run_data", "position_ID_sex_pilot2.csv"))


sex <- sex %>%
  mutate(Individuals_ID_well = paste0(Indviduals_ID, "_", Wellplate_location)) %>%
  select(Individuals_ID_well, Sex)

# join together

data_locomotion <- data_locomotion %>%
  left_join(sex, by = "Individuals_ID_well")

data_locomotion$Round_f <- as.factor(data_locomotion$round)
```

Create final dataset.

```{r}
data_locomotion <- data_locomotion %>%
  select(Individuals_ID_well, Individuals_ID, Datafile_ID, Date, arena_distance, Sex, Round_f)

data_locomotion$Sex <- as.factor(data_locomotion$Sex)
data_locomotion$Round_f <- as.factor(data_locomotion$Round_f)
head(data_locomotion)
```



## Example visualisations nad basic summaries

```{r}
data_locomotion_avg <- data_locomotion %>%
  group_by(Individuals_ID_well) %>%
  summarise(avg = mean(arena_distance))

# raw data variation
data_locomotion_avg %>%
  ggplot(aes(x = avg)) +
  geom_histogram() +
  theme_classic() + theme(text = element_text(size = 15)) +
  xlab("Mean arena distance")

# log scale
data_locomotion_avg %>%
  ggplot(aes(x = log(avg+1))) +
  geom_histogram() +
  theme_classic() + theme(text = element_text(size = 15)) +
  xlab("Mean log(arena distance)")

data_locomotion %>%
  ggplot(aes(x = Individuals_ID_well, y = log(arena_distance+1))) +
  geom_boxplot(width = 0, outlier.size = 0.7) + theme_classic() + theme(axis.text.x = element_blank()) +
  ylab("Arena distance") + xlab("Individuals")
```

We can also visualise sexual differences.

```{r}

#violin plot of distance grouped by sex
library(ggpubr)

violin <- na.omit(data_locomotion) %>% 
  group_by(Individuals_ID_well) %>% 
  summarise(arena_distance = log(mean(arena_distance) +1), Sex = unique(Sex)) %>% 
  ggviolin(., x = "Sex", y = "arena_distance", add = c("jitter", "mean_se"),
           error.plot = "crossbar", fill = 'skyblue') +
  labs(y = "log(Mean arena distance)")
violin
```


Simple linear model to explore undelrying variation.

```{r}
# model: variance between rounds, individuals and batches, fixed effect of sex and assay date
model1<- lmer(scale(log(arena_distance+1)) ~ Sex + as.factor(Date) + (1|Round_f) + (1|Individuals_ID_well) + (1|Datafile_ID),
             data = data_locomotion)
summary(model1)
qplot(residuals(model1)) + theme_classic()

# model: additional variation in sex effect between individuals (i.e. sex-specific between individual variance)
model2<- lmer(scale(log(arena_distance+1)) ~ Sex + as.factor(Date) + (1|Round_f) + (Sex-1|Individuals_ID_well) + (1|Datafile_ID),
             data = data_locomotion)

# marginally off convergence but let's proceed
summary(model2)$coef %>%
  kbl() %>%
  kable_material(c("striped"))

summary(model2)$varcor %>%
  kbl() %>%
  kable_material(c("striped"))
# males tend to have larger vairance in the reposne

anova(model1, model2) %>% kbl() %>% kable_material(c("striped"))
# difference in sex-specific variances significant (likelihood-ration test)
```



# Y-maze

## Data loading

First we load the y-maze data and tidy it up to separate summary data from zone changes data.

```{r}
## parse datafiles into a nested tibble
data_path <- here('Data/ymaze/')
data_files <- list.files(here('Data/ymaze/'), recursive = T)
data_files[1:8]
```

The below parser identifies lines in the dataset that directly relate to zone-switching data and extracts them, or (when `summary = TRUE`) it extract the arena distances summaries from the bottom section of the file.

```{r}
# helper functions extracting the data rows and the header rows
data_parser <- function(pattern, path, filename, summary = FALSE) {
  dat_temp <- readLines(paste(path, filename, sep = ''))
  zone_change_index <- grep(pattern, dat_temp)
  
  if(summary == FALSE) {
    return(read_delim(file = dat_temp[zone_change_index], col_names = F, delim = ',',
                      quote = '\"' # skip = 4, nrows = length(dat_temp) - 4 - 1)
    ))
  } else {
    return(read_delim(file = dat_temp[-zone_change_index], col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3))
  }
}

# test the parser
# here we simply check that the selected pattern to be looked for (`Arena`) is indeed present in the file.
pattern1 <- "Arena" # defines what should contain each line that we look for
test1 <- readLines(paste(data_path, data_files[1], sep = ''))
id_test <- grep(pattern1, test1) # use (simplified) RE to select lines
```

```{r}
# this extracts (messy) run and temporary data from each file
test_dat <- read_delim(file = test1[-id_test], col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3)

# helper function that parses the (messy) run data into a readable format)
head_parser <- function(path, filename) {
  dat_temp <- readLines(paste(path, filename, sep = ''), n = 4)
  return(read_delim(file = dat_temp,
                    delim = ',',
                    col_names = F))
}
```


The below code generates a nested tibble (grouped by single analysis output files, i.e., `Filename`).

```{r}
# create tibble with raw data (nested: data and header nested under file names)
dat_raw <- tibble(data_files = data_files) %>%
  mutate(data = map(data_files, 
                    ~ data_parser(pattern1,
                                  data_path,
                                  .x, summary = FALSE))) %>%
  mutate(head = map(data_files, ~ head_parser(data_path, .x)))

# this removes the redundant 'Arena' column
dat_raw[[2]] = dat_raw[[2]] %>%
  map(~ select(.x, !c(X3)))

# rename variables in sub-tibble based on their real content
dat_raw[[2]] = dat_raw[[2]] %>%
  map(~ rename(.x, time = X1, Info = X2, Arena = X4, Action = X5, Zone_no = X6))

# reformat the head sub-tibble
dat_raw[[3]] = dat_raw[[3]] %>%
  map(~ pivot_wider(.x, names_from = X3, values_from = X4)) %>%
  map(~ rename(.x, Datafile_ID = `Subject Identification`)) %>%
  map(~ select(.x, Apparatus, Datafile_ID))

# check the modifications
dat_raw[[2]][[1]]
dat_raw[[3]][[1]]
```

The below code processes arena movement summaries contained in the bottom sections of output files

```{r}
# parse summary data from a file into a separate tibble - this is not used for now
# processed only for consistency
dat_raw_summ = tibble(data_files = data_files) %>%
  mutate(data = map(data_files, ~ data_parser(pattern1,
                                data_path,
                                .x,
                                summary = T)))
dat_raw_summ[[2]] = dat_raw_summ[[2]] %>%
  map(~ select(.x, !c(X1, X2, X3))) %>%
  map(~ rename(.x, Summary_stat = X4))
dat_raw_summ[[2]][[1]]
```

Unnest the tibble and select relevant variables.

```{r}
dat_un <- dat_raw
dat_un = dat_raw %>%
  unnest(head) %>%
  unnest(data)

dat_un <- dat_un %>%
  select(data_files, Datafile_ID, time, Arena, Action, Zone_no) %>%
  rename(Data_file = data_files)

```

Turn into wider format with enter/exit columns, add row ID to maintain order if needed.

```{r}
dat_un <- dat_un %>%
  mutate(Row_ID = row_number()) %>%
  pivot_wider(names_from = Action, values_from = Zone_no) %>%
  rename(Exit_zone = Exit_Zone, Enter_zone = Enter_Zone)
dat_un
```

Bin into 10-min intervals (if binning would be needed or only first 10 mins of exploration used). Then sort the dataset into individual arenas and chronologically within arenas, and then in enter-exit order at the lowest level.

```{r}
dat_an <- dat_un

dat_an <- dat_an %>%
  mutate(Bin = ifelse(time > 600 & time < 1200, 1,
                      ifelse(time > 1200 & time < 1800, 2,
                             ifelse(time > 1800 & time < 2500, 3, NA))))

dat_an <- dat_an %>%
  arrange(Data_file, Datafile_ID, Arena, time, Exit_zone)
dat_an
```

Error checking - do all enter zones have a matching exit zone (if enter and exit exist)?

```{r}
dat_an <- dat_an %>%
  mutate(Zone = ifelse(Enter_zone == lead(Exit_zone), Enter_zone, 666)) %>%
  mutate(t_enter = ifelse(Enter_zone >= 1, time, 666)) %>%
  mutate(t_exit = ifelse(Exit_zone >= 1, time, 666))

dat_an %>% filter(Zone == 666)
dat_an %>% filter(t_enter == 666)
dat_an %>% filter(t_exit == 666)
dat_an[86160:86180,] -> dat_ana
# ALL GOOD! Note - this step is v. important and serves to test
# if data points were sorted correctly
# Single detected mistakes are likely final entries that failed to exit before assay end

# Let's filter out the non-conforming cases and re-confirm the rest is ordered correctly
dat_an <- dat_an %>% filter(Zone != 666)
dat_an <- dat_an %>%
  arrange(Data_file, Datafile_ID, Arena, time, Exit_zone)
dat_an <- dat_an %>%
  mutate(Zone = ifelse(Enter_zone == lead(Exit_zone), Enter_zone, 666)) %>%
  mutate(t_enter = ifelse(Enter_zone >= 1, time, 666)) %>%
  mutate(t_exit = ifelse(Exit_zone >= 1, time, 666))
dat_an %>% filter(Zone == 666)
```

Simplify data by putting enter and exit data in one row:

```{r}
dat_an <- dat_an %>%
  filter(is.na(Zone) | Zone != 666) %>%
  select(Data_file, Datafile_ID, time, Arena, Row_ID, Bin, Zone, t_enter, t_exit)
dat_an <- dat_an %>%
  mutate(t_exit = lead(t_exit))
dat_an <- na.omit(dat_an)
dat_an <- dat_an %>%
  mutate(t_zone = t_exit - t_enter)

dat_an
```

Remove center zone (middle of the Y-maze).

```{r}
dat_an2 <- dat_an %>% filter(zone != 4)
dat_an2
```

We need to work separately for each individual - the easiest is to separate individuals into their own subtibbles and work on them by vectorizing operations - to achieve this we have to identify each individual (by pasting together file name and arena).

```{r}
well_arena <- read_delim(here('Data', 'sex', 'well_arena_corresp.csv'), delim = ';')

names(dat_an2)[2] <- "Datafile_ID"
dat_an2 <- dat_an2 %>%
  left_join(select(run_register, Datafile_ID, Individuals_ID, Plate, Unit_ID), by = "Datafile_ID") %>%
  mutate(ymaze_arena = paste0(Plate, "_", arena)) %>%
  # mutate(fly_id = paste(gsub('[A-Za-z0-9]+\\/([A-Za-z0-9_-]+)\\.csv', '\\1', data_files), arena, sep = "_"))
  mutate(fly_id = paste0(Datafile_ID, "_", ymaze_arena)) %>%
  left_join(well_arena, by = "ymaze_arena")

# split datafile into individual flies
dat_an2 <- dat_an2 %>% split(., .[, "fly_id"])

dat_an2 <- dat_an2 %>%
  map(~ mutate(.x, lag_zone = lag(zone))) %>%
  map(~ mutate(.x, turn = case_when(lag_zone==1 & zone==2 ~ 'L',
                                  lag_zone==1 & zone==3 ~ 'R',
                                  lag_zone==2 & zone==1 ~ 'R',
                                  lag_zone==2 & zone==3 ~ 'L',
                                  lag_zone==3 & zone==1 ~ 'L',
                                  lag_zone==3 & zone==2 ~ 'R',
                                  # lag_zone==zone ~ 'X',
                                  TRUE~ NA_character_ ))) %>%
  map(~ select(.x, data_files, Datafile_ID, fly_id, time, ymaze_arena, Individuals_ID, plate48well, bin, zone, t_zone, turn))

dat_an3 <- bind_rows(dat_an2)
dat_an3 <- dat_an3 %>%
  arrange(data_files, Datafile_ID, fly_id, bin)

dat_an4 <- na.omit(dat_an3)
dat_an4
```

Analysis of tetragrams.

```{r}
dat_tet <- dat_an4 %>%
  group_by(fly_id)

dat_tet <- dat_tet %>%
  mutate(tetragram = str_c(turn, lead(turn), lead(turn,2), lead(turn,3))) %>%
  ungroup() %>%
  select(fly_id, turn, tetragram)
dat_tet
```

We can now summarise tetragrams and turns:

```{r}
all_tetragrams <- unique(dat_tet$tetragram)

tetra_long <- dat_tet %>%
  select(-turn) %>%
  na.omit() %>%
  group_by(fly_id) %>%
  table() %>%
  as_tibble() %>%
  arrange(fly_id)

tetra_wide = tetra_long %>% pivot_wider(names_from = tetragram, values_from = n)

turn_long = dat_tet %>%
  select(-tetragram) %>%
  na.omit() %>%
  group_by(fly_id) %>%
  table() %>%
  as_tibble() %>%
  arrange(fly_id)

turn_wide = turn_long %>% pivot_wider(names_from = turn, values_from = n)

data = tetra_wide %>%
  left_join(turn_wide, by = c("fly_id")) %>%
  mutate(total_turns = L+R,
         reps = LLLL + RRRR,
         alter = RLRL + LRLR,
         rel_reps = (reps*100)/total_turns,
         rel_alter = (alter*100)/total_turns,
         rel_R = (R*100)/total_turns,
         rel_L = (L*100)/total_turns)

data
```

## Linking to sex data - NOT FINISHED

```{r}


data <- data %>%
  left_join(run_register)

```

## Example visualisations of y-maze data

```{r}

tetra_long %>%
  filter(n > 0) %>%
  ggplot(aes(x = tetragram, y = n)) +
  geom_bar(stat = "sum") +
  theme_classic() + labs(x = 'Tetragram', y = 'Total number') +
  theme(legend.position = "none", text = element_text(size = 15))



```